import requests
from bs4 import BeautifulSoup
import csv
import telebot
import config
import proxy_moule
import random_user_agent

bot = telebot.TeleBot(config.TOKEN)

URL = 'https://www.avito.ru/'

# FILE = 'users_results/'
message_id_to_del = []


# –§—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è html —Å—Ç—Ä–∞–Ω–∏—Ü—É [params - –Ω–µ–æ—Ö–æ–¥–∏–º –¥–ª—è –¥–æ–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫ –ø—Ä–∏–º–µ—Ä—É –¥–ª—è –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã]
def get_html(url, params=None):
    r = requests.get(url, headers=random_user_agent.random_headers(), params=params)
    return r


def get_html_proxy(url, proxy, params=None):
    r = requests.get(url, headers=random_user_agent.random_headers(), params=params, proxies={'http': proxy})
    return r


def get_content(html):
    # –ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä —Å–∞–º–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞, 2 - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π  'html.parser'
    soup = BeautifulSoup(html, 'html.parser')
    items = soup.find_all('div',
                          class_='description item_table-description')  # soup.find('a', class_='cover anime-tooltip-processed').find_next('span').get_text()
    collection = []
    for item in items:
        metro_distance = item.find('span', class_='item-address-georeferences-item__after')
        if metro_distance:
            metro_distance = metro_distance.get_text()
        else:
            metro_distance = ' –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
        metro = item.find('span', class_='item-address-georeferences-item__content')
        if metro:
            metro = metro.text
        else:
            metro = '–º–µ—Ç—Ä–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ'
        collection.append({
            'vacancy': item.find('a', class_='snippet-link').find_next('span').get_text(strip=True).replace('\n ', ''),
            'salary': item.find('div', class_='snippet-price-row').find_next('span').get_text(strip=True).replace(
                '\n ', ''),
            'distance': metro + ', ' + metro_distance.replace('\xa0', ' '),
            'link': 'https://www.avito.ru/' + item.find('a', class_='snippet-link').get('href')
        })
    return collection


def get_pages_count(html):
    soup = BeautifulSoup(html, 'html.parser')
    pagination = soup.find_all('span', class_='pagination-item-1WyVp')
    count = 0
    if pagination:
        for item in pagination:
            count += 1
        return int(pagination[count - 2].get_text())
    else:
        return 1


def save_file(items, path):
    with open(path, 'w', newline='',
              encoding='utf-8') as file:  # –ø—Ä–∏ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ with open —Ñ–∞–π–ª –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –µ—Å–ª–∏ –∫—Ç–æ –Ω–µ –∑–Ω–∞–ª –û_–æ
        writer = csv.writer(file, delimiter=';')  # –†–∞–∑–¥–µ–ª—è—é –∏ –≤–ª–∞—Å—Ç–≤—É—é –≤ –µ–∫—Å–µ–ª–µ ;
        writer.writerow(['–í–∞–∫–∞–Ω—Å–∏—è', '–ó–∞—Ä–ø–ª–∞—Ç–∞', '–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç –º–µ—Ç—Ä–æ',
                         '–°—Å—ã–ª–∫–∞'])  # –î–∞–Ω–Ω—ã–µ –±–ª–∏–Ω –ø–µ—Ä–µ–¥–∞—é)))
        for item in items:
            writer.writerow([item['vacancy'], item['salary'], item['distance'], item['link']])


def main_parse(message, fname, proxy=None):
    if proxy is None:
        html = get_html(URL)
    else:
        html = get_html_proxy(URL, proxy=proxy)

    collection = []
    pages_count = get_pages_count(html.text)
    for page in range(1, pages_count + 1):
        # if page != 1:
        #     bot.delete_message(message.chat.id, message_id_to_del[-1] - 1)

        if page != 1:
            bot.edit_message_text(f'–°–∫–∞–Ω—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} –∏–∑ {pages_count}', message.chat.id, message_id_to_del[-1])
        elif page == 1:
            message_id_to_del.append(bot.send_message(message.chat.id, f'–°–∫–∞–Ω—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} –∏–∑ {pages_count}').message_id) # TODO edit_message_text

        # print(message_id_to_del[-1])
        #bot.send_message(message.chat.id, f'–°–∫–∞–Ω—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} –∏–∑ {pages_count}')
        #print(message_id_to_del[-1])
        if proxy is None:
            html = get_html(URL, params={'page': page})
        else:
            html = get_html_proxy(URL, proxy=proxy, params={'page': page})

        collection.extend(
            get_content(html.text))  # –ù—É —Ç–∏–ø–∞ –±–µ—Ä—É –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Ö—Ç–º–ª –∏ –ø–∞—Ä—Å—é –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞–±–µ–ª—å–Ω—ã–π –≤–∏–¥

    FILE = f"users_results/{fname}.csv"
    save_file(collection, FILE)
    bot.send_message(message.chat.id, f'–û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ {len(collection)} –≤–∞–∫–∞–Ω—Å–∏–π')


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ –≤—Å–µ—Ö.... –ß–µ–≥–æ –≤—Å–µ—Ö —Ç–æ? –ß—Ç–æ –æ–Ω–∞ –≤—ã–∑—ã–≤–∞–µ—Ç –±–ª—è—Ç—å? –£–º—Å—Ç–≤–µ–Ω–Ω—É—é –æ—Ç—Å—Ç–∞–ª–æ—Å—Ç—å? –ù–∞—Ö—É—è —è –≤–æ–æ–±—â–µ —ç—Ç–æ –ø–∏—à—É....
def parse(message, fname):

    #proxys = proxy_moule.get_connection() # get proxy from list
    bot.send_message(message.chat.id, '–û–±–Ω–æ–≤–ª—è—é —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...\n'
                                      '–ï—Å–ª–∏ —è –Ω–µ —Å–¥–µ–ª–∞—é —ç—Ç–æ, —Ç–æ —É –º–µ–Ω—è –≤–Ω—É—Ç—Ä–∏ –≤—Å–µ –ø–æ–ª–æ–º–∞–µ—Ç—Å—è –ø–æ–Ω–∏–º–∞–µ—à—å? –û_–û\n'
                                      '–ü–æ—ç—Ç–æ–º—É –ø–æ–¥–æ–∂–¥–∏ –Ω–µ–º–Ω–æ–∂–µ—á–∫–æ...')
    try:
        proxy_moule.proxy_work.upd_proxylist_main() # uodate proxy list function
    except Exception as e:
        print(f'Method({parse.__name__})\n'
              f'Update error\n'
              f'Error:\n{e}\n'
              f'========================')

    for proxy in proxy_moule.proxy_work.get_proxylist_main(): # proxy_moule.get_connection()
        html = get_html_proxy(URL, proxy=proxy)

        # TODO –°—é–¥–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–∫–æ–ª —Å –≥–∏—Ñ–∫–æ–π

        # print(f'{proxy} –°–Æ–î–ê –°–ú–û–¢–†–ò ')
        # print(proxy_moule.proxy_work.from_file_proxy_list[0])

        # –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è –ü–û–°–û–û–û–û–û–û–û–û–°–ò–ò–ò–ò–ò–ò–ò–ò)))
        if proxy != proxy_moule.proxy_work.from_file_proxy_list[0]:
            bot.edit_message_text(f'–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ {proxy}\n–°—Ç–∞—Ç—É—Å –∫–æ–¥: {html.status_code}', message.chat.id, message_id_to_del[-1])
        elif proxy == proxy_moule.proxy_work.from_file_proxy_list[0]:
            message_id_to_del.append(bot.send_message(message.chat.id, f'–ü–æ–¥–∫–ª—é—á–∞—é—Å—å –∫ {proxy}\n–°—Ç–∞—Ç—É—Å –∫–æ–¥: {html.status_code}').message_id)

        # —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è —Å–∞–∫ —Å–æ–º–µ –¥–∏–∫ –û_–æ
        # bot.send_message(message.chat.id, f'Conecting to {proxy}\n'
        #                                   f'Status code {html.status_code}')

        if html.status_code == 200:
            bot.send_message(message.chat.id,
                             "–û–ø, —è –ø–æ–¥–∫–ª—é—á–∏–ª—Å—è –∫ –ø—Ä–æ–∫—Å–∏ üòé\n"
                             "–¢–µ–ø–µ—Ä—å —Ç–æ —è –±—ã—Å—Ç—Ä–µ–Ω—å–∫–æ –æ–ø—Ä–∏—Ö–æ–¥—É—é –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏...")
            main_parse(message, fname, proxy=proxy)
            return True
    return False